{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation webpage of the HORDA cluster. Section Description Getting started Basic information about the account setup and connections and data transfer. Running calculations Information on jobs scheduling, SLURM and best practices of interactive work. FAQ Frequently asked questions and solutions to common problems. Available resources Technical description of available resources. Available software List of preinstalled software packages available to all users.","title":"Home"},{"location":"faq/","text":"How to run Jupyter notebooks remotely on HORDA? Assuming jupyter is running on troll-1 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@troll-1.sih-60.internal Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888 How to install python packages on HORDA? python3 (3.10 and 3.11) as well as python2 (2.7.18) along with the recent pip and venv are installed system-wide on each node of HORDA. You can simply install packages either with pip install --user or use venv or svirtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory. I want to use program XXX on HORDA, can I install it on my own? You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"FAQ"},{"location":"faq/#how-to-run-jupyter-notebooks-remotely-on-horda","text":"Assuming jupyter is running on troll-1 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@troll-1.sih-60.internal Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888","title":"How to run Jupyter notebooks remotely on HORDA?"},{"location":"faq/#how-to-install-python-packages-on-horda","text":"python3 (3.10 and 3.11) as well as python2 (2.7.18) along with the recent pip and venv are installed system-wide on each node of HORDA. You can simply install packages either with pip install --user or use venv or svirtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory.","title":"How to install python packages on HORDA?"},{"location":"faq/#i-want-to-use-program-xxx-on-horda-can-i-install-it-on-my-own","text":"You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"I want to use program XXX on HORDA, can I install it on my own?"},{"location":"first_steps/","text":"Setting up an account In order to create your an account on the HORDA cluster please contact Maciek or Pawel via e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow you to login to the entry node (jumphost-60) at sih-60.cent.uw.edu.pl and from there to login to the HORDA cluster front-node (ssh horda) and use the compute nodes ogr-[0-1] and troll-[1] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not cluster front node or compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-60] prefix in the message title and add cluster administrators @Maciek and @Pawel in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login. Connecting via SSH Connections to the HORDA cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@sih-60.cent.uw.edu.pl This will bring you to the entry node (jumphost-60) , afterwards you can connect to the horda front node and submit tasks to the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@horda In order to simplify file copying and every day work the suggested way of connecting to the horda cluster is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@sih-60.cent.uw.edu.pl 10 .10.60.1/24 Once connection is established you can directly login to the horda front node or compute nodes using: ssh your_username@horda.sih-60.internal Information Additionally, depending on your computer and network settings, you may have to connect to horda nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here Work environment Each user has access to two personal directories: /home/users/your_username /workspace/your_username Warning These folders are shared between all nodes of the HORDA cluster but not with the jumphost-60. Please don't use the jumphost to store large volumes of data. Transferring files The recommended options to send or fetch files from HORDA cluster are either scp or rsync . The storage on the entry host sih-60.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly to the shared space on the horda cluster. Information Assuming you established a connection with sshuttle you can directly send files or directories to the cluster: scp file.txt your_username@horda.sih-60.internal:~/ Next steps Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Getting started"},{"location":"first_steps/#setting-up-an-account","text":"In order to create your an account on the HORDA cluster please contact Maciek or Pawel via e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow you to login to the entry node (jumphost-60) at sih-60.cent.uw.edu.pl and from there to login to the HORDA cluster front-node (ssh horda) and use the compute nodes ogr-[0-1] and troll-[1] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not cluster front node or compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-60] prefix in the message title and add cluster administrators @Maciek and @Pawel in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login.","title":"Setting up an account"},{"location":"first_steps/#connecting-via-ssh","text":"Connections to the HORDA cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@sih-60.cent.uw.edu.pl This will bring you to the entry node (jumphost-60) , afterwards you can connect to the horda front node and submit tasks to the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@horda In order to simplify file copying and every day work the suggested way of connecting to the horda cluster is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@sih-60.cent.uw.edu.pl 10 .10.60.1/24 Once connection is established you can directly login to the horda front node or compute nodes using: ssh your_username@horda.sih-60.internal Information Additionally, depending on your computer and network settings, you may have to connect to horda nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here","title":"Connecting via SSH"},{"location":"first_steps/#work-environment","text":"Each user has access to two personal directories: /home/users/your_username /workspace/your_username Warning These folders are shared between all nodes of the HORDA cluster but not with the jumphost-60. Please don't use the jumphost to store large volumes of data.","title":"Work environment"},{"location":"first_steps/#transferring-files","text":"The recommended options to send or fetch files from HORDA cluster are either scp or rsync . The storage on the entry host sih-60.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly to the shared space on the horda cluster. Information Assuming you established a connection with sshuttle you can directly send files or directories to the cluster: scp file.txt your_username@horda.sih-60.internal:~/","title":"Transferring files"},{"location":"first_steps/#next-steps","text":"Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Next steps"},{"location":"resources/","text":"The HORDA cluster has in total 92 physical CPUs (184 virtual), 16 GPUs, 576 GB of RAM and around 140 TB of storage. Characteristcs of available compute nodes are summarized below: hostname Cores CPU RAM local free space (/tmp) GPU year ogr-0 24 2x Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz 64GB ~ 70 GB, ssd - - ogr-1 24 2x Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz 64GB ~ 70 GB, ssd - - troll-1 16 2x Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz 32GB ~ 800 GB, sata 2x GeForce GTX 780 Ti -","title":"Available resources"},{"location":"rules/","text":"Currently available only in Polish. English version coming soon. Poni\u017csze zasady dotycz\u0105 has\u0142a do w\u0119z\u0142a dost\u0119powego sih-60.cent.uw.edu.pl . U\u017cytkownik jest zobowi\u0105zany do ochrony swojego has\u0142a. Has\u0142o ma charakter poufny. Zabronione jest udost\u0119pnianie ujawnianie has\u0142a w jakikolwiek spos\u00f3b (np. przekazanie innej osobie, zapisywanie w formie nie szyfrowanej, pozostawianie na widoku). W przypadku podejrzenia ujawnienia has\u0142a u\u017cytkownik zobowi\u0105zany jest do jego natychmiastowej zmiany oraz przekazania informacji o zdarzeniu do administrator\u00f3w klastra oraz Dzia\u0142u IT CeNT UW. U\u017cytkownik ponosi odpowiedzialno\u015b\u0107 za wszelkie czynno\u015bci wykonane za po\u015brednictwem konta do niego przypisanego zabezpieczonego jego has\u0142em. Tworz\u0105c has\u0142o nale\u017cy unika\u0107: przyjmowania regularnych schemat\u00f3w w tworzeniu hase\u0142, wybierania sekwencji b\u0105d\u017a powt\u00f3rze\u0144 znak\u00f3w \u0142atwych do podejrzenia, korzystania z nazwy w\u0142asnej u\u017cytkownika, wa\u017cnych dla u\u017cytkownika dat, imion, numer\u00f3w telefon\u00f3w kom\u00f3rkowych, numer\u00f3w rejestracyjnych aut. Zasady tworzenia hase\u0142: Has\u0142o powinno mie\u0107 co najmniej 10 znak\u00f3w (zalecane minimum 15), has\u0142o powinno zawiera\u0107 wielkie i ma\u0142e litery, znaki specjalne, cyfry (0 - 9). Zabrania si\u0119: podawania has\u0142a w wiadomo\u015bciach e-mail, b\u0105d\u017a te\u017c w odpowiedzi na \u017c\u0105danie, kt\u00f3re zosta\u0142o przes\u0142ane poczt\u0105 e-mail, wpisywania hase\u0142 do komputer\u00f3w powstaj\u0105cych w u\u017cytku publicznym np. komputery w kafejkach internetowych, hotelach, bibliotekach itd, zapisywania hase\u0142 w plikach niezaszyfrowanych. U\u017cytkownik zobligowany jest do zmiany has\u0142a nadanego przez administratora przy pierwszej pr\u00f3bie logowania. Zmiana has\u0142a dost\u0119powego do w\u0119z\u0142a dost\u0119powego wymuszana jest co 90 dni. Informacje o utracie has\u0142a, konieczno\u015bci jego zresetowania oraz inne problemy techniczne powinny by\u0107 zg\u0142aszane na adres dzia\u0142u IT CeNT UW - it @ cent.uw.edu.pl do\u0142\u0105czaj\u0105c w CC administrator\u00f3w klasta. Temat wiadomo\u015bci powinien rozpoczyna\u0107 si\u0119 od prefiksu [sih-60] .","title":"General rules"},{"location":"running_jobs/","text":"Introduction Jobs (both batch and interactive sessions) on HORDA should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - ogr (cpu only nodes) and troll (gpu nodes) or all (all nodes). The time of execution is limited to 28 days on each partition. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 troll interact username R 11 -18:07:21 1 troll-1 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up 28 -00:00:0 3 idle ogr- [ 0 -1 ] ,troll-1 troll up 28 -00:00:0 1 idle troll-1 ogr up 28 -00:00:0 2 idle ogr- [ 0 -1 ] Interactive sessions HORDA can be used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. You can either start an interactive session using srun or allocate resources using the salloc utility and then ssh into that host and work there. You can tweak your allocation depending on work needs, see the following table for details and examples. Both commands have a similar set of options: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) --gres Number of GPUs allocated for the job (_default = None, --gres=gpu, --gres=gpu:2) --mem Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Specifi host or hosts to get your resources (i.e. troll-1) Example To login interactively to troll-1 with 2 gpus and 8 cores and a total of 12 GB of memory : srun -n 8 -w troll-1 --gres = gpu:2 --mem = 12 --pty bash To obtain an allocation on troll-1 with 2 gpus and 8 cores and a total of 12 GB of memory : salloc -n 8 -w troll-1 --gres = gpu:2 --mem = 12 Important! Please remember to quit your interactive allocation when you're done with your work. You can do it by simply typing exit (or CTRL+D). Batch jobs Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p troll # troll partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Running calculations"},{"location":"running_jobs/#introduction","text":"Jobs (both batch and interactive sessions) on HORDA should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - ogr (cpu only nodes) and troll (gpu nodes) or all (all nodes). The time of execution is limited to 28 days on each partition. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 troll interact username R 11 -18:07:21 1 troll-1 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up 28 -00:00:0 3 idle ogr- [ 0 -1 ] ,troll-1 troll up 28 -00:00:0 1 idle troll-1 ogr up 28 -00:00:0 2 idle ogr- [ 0 -1 ]","title":"Introduction"},{"location":"running_jobs/#interactive-sessions","text":"HORDA can be used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. You can either start an interactive session using srun or allocate resources using the salloc utility and then ssh into that host and work there. You can tweak your allocation depending on work needs, see the following table for details and examples. Both commands have a similar set of options: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) --gres Number of GPUs allocated for the job (_default = None, --gres=gpu, --gres=gpu:2) --mem Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Specifi host or hosts to get your resources (i.e. troll-1) Example To login interactively to troll-1 with 2 gpus and 8 cores and a total of 12 GB of memory : srun -n 8 -w troll-1 --gres = gpu:2 --mem = 12 --pty bash To obtain an allocation on troll-1 with 2 gpus and 8 cores and a total of 12 GB of memory : salloc -n 8 -w troll-1 --gres = gpu:2 --mem = 12 Important! Please remember to quit your interactive allocation when you're done with your work. You can do it by simply typing exit (or CTRL+D).","title":"Interactive sessions"},{"location":"running_jobs/#batch-jobs","text":"Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p troll # troll partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Batch jobs"},{"location":"software/","text":"Name Version Hosts Location Maintainer amber (with ambertools) 18 all /opt/amber18/* @Pawel gromacs 2023.1 all /opt/gromacs-2023/* @Pawel","title":"Available software"}]}